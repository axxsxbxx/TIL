# 텍스트마이닝

## 1. 텍스트마이닝

- 단어의 출현 빈도, 단어 간 관계성 등을 파악하여 유의미한 정보를 추출하는 것
- `자연어 처리 기술`을 기반으로 하고 있음
- 텍스트 데이터를 중점적으로 다루는 데이터마이닝이라고 보면 됨
- 형태소 분석기나 구문 분석기와 같은 자연어 처리 도구를 잘 사용할 수 있어야 함
- 소셜 미디어의 텍스트 마이닝은 많은 양의 비정형 데이터를 분석함으로써 해당 브랜드나 제품에 대한 다양한 의견과 감성 반응을 살펴볼 수 있음
- 문서들을 자동으로 분류할 수 있고, 문서나 단어들 간의 연관성을 분석할 수 있으며 텍스트에 담겨 있는 감성을 예측할 수 있고 시간의 흐름에 따른 이슈들의 변환 과정을 추적할 수 있음



## 2. 자연어 처리

- `자연어` : 일상생활에서 사용하는 말, 언어
- 자연어 처리 기술을 바탕으로 사람들이 작성한 텍스트를 컴퓨터가 분석하여 중요한 단어나 문장들을 추출할 수 있음

> - 형태소 분석기 : 형태소를 구분하고 무엇인지 알려줌
> - 구문 분석기 : 품사보다는 단위가 더 높은 논리적 레벨까지 처리해줌



## 3. KoNLP 패키지

> 형태소 분석으로 어절들의 품사를 파악한 후 명사, 동사, 형용사 등 의미를 지닌 품사의 단어를 추출해 각 단어가 얼마나 많이 등장했는지 확인함

```R
# 명사 추출
extractNoun(text)
sapply(text, extractNoun, USE.NAMES=F)

# 형태소 분석
SimplePos22(text)
SimplePos09(text)
```

| 축약어 | 의미                     |
| ------ | ------------------------ |
| `F`    | 외국어                   |
| `N`    | 체언(명사, 대명사, 수사) |
| `P`    | 용언(동사, 형용사)       |
| `M`    | 수식언(관형사, 부사)     |
| `I`    | 독립언(감탄사)           |
| `J`    | 관계언(조사)             |
| `E`    | 어미                     |
| `X`    | 접사                     |



## 4. tm 패키지

- 텍스트 데이터의 정제 작업을 지원하는 다양한 변환 함수를 제공함

```R
# 사용 가능한 변환함수의 리스트를 확인할 수 있음
getTransformations()

# 변환 작업을 처리함
# 문장 부호를 제거하거나, 문자를 모두 소문자로 바꾸거나, 단어의 어간을 추출해줌
tm_map(
	x,		# 코퍼스
    FUN		# 변환에 사용할 함수
)

tm_map(corp1,stripWhitespace) # 여러 개의 공백을 하나의 공백으로 변환함
tm_map(corp2.,removeNumbers) # 숫자를 제거함
tm_map(myCorpus, content_transformer(tolower)) # 영문 대문자를 소문자로 변환함
tm_map(corp2.,removePunctuation) # 마침표,콤마,세미콜론,콜론 등 문자 제거함
tm_map(corp2.,PlainTextDocument)

stopword2. <- c(stopwords('en'),"and","but") # 기본 불용어 외에 불용어로 쓸 단어 추가
tm_map(corp2.,removeWords,stopword2.) # 불용어 제거하기 (전치사 , 관사 등)   
```



- 텍스트 마이닝을 위해 수행해야 할 첫 번째 작업은 비정형 텍스트를 구조화된 데이터로 변환하는 것.

- 텍스트를 구조화하는 방법 가운데 하나로 `코퍼스(corpus) 접근법`이 일반적으로 많이 사용됨

  > 코퍼스(corpus) : 언어학에서 구조를 이루고 있는 텍스트 집합
  >
  > 코퍼스 접근법 : 분석 대상이 되는 문서를 단어의 집합으로 단순화시킨 표현 방법
  >
  > - 단어의 순서나 문법은 무시하고 단어의 출현 빈도만을 이용하여 텍스트를 매트릭스로 표현함
  > - 이 때 생성되는 매트릭스를 term-document-matrix(TDM) 또는 document-term-matrix(DTM)이라고 함

- `단어 가중치` : 문서에서 어떤 단어의 중요도를 평가하기 위해 사용되는 통계적인 수치

  > TF : Term Frequency(단어 빈도)
  >
  > IDF : Inverse Document Frequency(역 문서 빈도)
  >
  > DF : Document Frequency(문서 빈도)
  >
  > TFIDF : TF * IDF
  >
  > - 특정 문서 내에서 단어 빈도가 높을수록, 전체 문서들엔 그 단어를 포함한 문서가 적을수록 TFIDF 값이 높아지게 됨. 즉, 문서 내에서 해당 단어의 중요도는 커지게 됨

```R
# 사용 가능한 소스객체의 종류 파악
getSources()

# 다양한 소스로부터 읽어들인 텍스트를 텍스트 마이닝을 위한 Corpus 객체로 변환함
Corpus()

# 단어 가중치
as.matrix(weightTf(tdm))
as.matrix(weightTfIdf(tdm))
```



## 5. proxy 패키지

- 문서간 유사도 분석 : 문서들 간에 동일한 단어 또는 비슷한 단어가 얼마나 공통으로 많이 사용 되었는지 분석하는 것

- 문서 간 단어들의 차이를 계산함

  > `코사인 유사도(Cosine Similarity)`
  >
  > : 두 벡터 간의 코사인 각도를 이용하여 유사도를 측정함
  >
  > : 두 벡터의 값이 완전 동일하면 1, 반대 방향이면 -1, 90도의 각을 이루면 0이 됨
  >
  > : 1에 가까울수록 유사도가 높음
  >
  > `유클리드 거리(Euclidean distance)`
  >
  > : 피타고라스의 정리를 통해 두 점 사이의 거리를 구하는 것과 동일함

```R
# 코사인 거리(Cosine Distance) : '1 - 코사인 유사도(Cosine Similarity)'
dist(com, method = "cosine") 

# 유클리드 거리
dist(com, method = "Euclidean") 
```



## 6. 텍스트 마이닝의 결과 시각화

### wordcloud 패키지

> 단어의 개수를 세어 개수의 크기 값에 따라서 단어의 크기를 차등적으로 출력하여 키워드가 되는 단어를 좀 더 강조하여 출력하는 시각화

```R
wordcloud(words,freq,
          scale=c(4,.5),	# 빈도가 가장 큰 단어와 가장 작은 단어 폰트 사이의 크기
          min.freq=3, max.words=Inf, # 빈도 3이상의 단어 표현
          random.order=TRUE, 	# True(랜덤배치)/False(빈도수가 큰단어를 중앙에 배치)
          random.color=FALSE, 	# True(색상랜덤)/False(빈도수 순으로 색상표현)
          rot.per=.1, 		# 90도 회전해서 보여줄 단어 비율
          colors="black",	# 색상 이름
          ordered.colors=FALSE,
          use.r.layout=FALSE, 
          fixed.asp=TRUE, ...)

# WordCloud 결과를 이미지 파일로 저장
savePlot(szWordCloudImageFile, type="png") 
```



### qgraph 패키지

> 동시출현이란 한 문장, 문단 또는 텍스트 단위에서 같이 출현한 단어를 가리킴
>
> - 단어의 연결성을 찾는데 활욛됨
>
> 동시출현 네트워크 : 특정 텍스트 단위에서 공동으로 출현한 단어의 집합적 상호 연결을 표현하는 방식
>
> - 나타나는 단어를 모두 표시한 뒤, 둘 사이를 선으로 연결해 나가다 보면 단어의 네트워크를 만들 수 있음

